{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T02:53:57.676097Z",
     "iopub.status.busy": "2025-07-18T02:53:57.675498Z",
     "iopub.status.idle": "2025-07-18T02:58:27.515837Z",
     "shell.execute_reply": "2025-07-18T02:58:27.514740Z",
     "shell.execute_reply.started": "2025-07-18T02:53:57.676076Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install vllm\n",
    "!pip install bitsandbytes>=0.46.1\n",
    "# !pip install --upgrade --no-deps vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T02:58:27.517693Z",
     "iopub.status.busy": "2025-07-18T02:58:27.517444Z",
     "iopub.status.idle": "2025-07-18T02:58:30.681071Z",
     "shell.execute_reply": "2025-07-18T02:58:30.680344Z",
     "shell.execute_reply.started": "2025-07-18T02:58:27.517668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T02:58:30.682363Z",
     "iopub.status.busy": "2025-07-18T02:58:30.682049Z",
     "iopub.status.idle": "2025-07-18T02:58:37.257049Z",
     "shell.execute_reply": "2025-07-18T02:58:37.256271Z",
     "shell.execute_reply.started": "2025-07-18T02:58:30.682328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!gdown 1EiJmjVmq54zrH2n2h9yUbpktkJKlYwKF -O data/alqac25_train.json\n",
    "!gdown 1GUbdSU-ls_THuU-LStkFR6nA3dEdaG2X -O data/alqac25_law.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T02:58:37.259162Z",
     "iopub.status.busy": "2025-07-18T02:58:37.258933Z",
     "iopub.status.idle": "2025-07-18T03:03:09.433866Z",
     "shell.execute_reply": "2025-07-18T03:03:09.433033Z",
     "shell.execute_reply.started": "2025-07-18T02:58:37.259139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "max_seq_length = 8192\n",
    "# think_model_name = \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\"\n",
    "think_model_name = \"unsloth/DeepSeek-R1-0528-Qwen3-8B-unsloth-bnb-4bit\"\n",
    "# think_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
    "think_model = LLM(\n",
    "        model=think_model_name,\n",
    "        gpu_memory_utilization=0.99,\n",
    "        max_model_len = max_seq_length,\n",
    "        tensor_parallel_size=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:03:09.434863Z",
     "iopub.status.busy": "2025-07-18T03:03:09.434621Z",
     "iopub.status.idle": "2025-07-18T03:03:09.438738Z",
     "shell.execute_reply": "2025-07-18T03:03:09.437963Z",
     "shell.execute_reply.started": "2025-07-18T03:03:09.434846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# max_input_tokens = len(tokenizer(prompt)[\"input_ids\"])\n",
    "# max_new_tokens = max_seq_length - max_input_tokens\n",
    "\n",
    "# max_new_tokens = 2048\n",
    "max_new_tokens = 4096\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 0.6, top_p = 0.95, top_k = 20, # For thinking\n",
    "    max_tokens=max_new_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:03:09.439836Z",
     "iopub.status.busy": "2025-07-18T03:03:09.439627Z",
     "iopub.status.idle": "2025-07-18T03:03:52.250351Z",
     "shell.execute_reply": "2025-07-18T03:03:52.249524Z",
     "shell.execute_reply.started": "2025-07-18T03:03:09.439820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_think_response(history: list[dict]) -> str: \n",
    "    response = think_model.chat(messages=history, sampling_params=sampling_params)\n",
    "    response_text = response[0].outputs[0].text\n",
    "    return response_text\n",
    "\n",
    "system_prompt = \"\"\"Bạn là một trợ lý pháp lý Tiếng Việt, có nhiệm vụ trả lời các câu hỏi ngắn gọn, chính xác, dựa trên nội dung điều luật được cung cấp. Chỉ trả lời bằng Tiếng Việt.\n",
    "\n",
    "Bạn hãy trả lời theo định dạng sau:\n",
    "<think>\n",
    "[Suy nghĩ, phân tích của bạn]\n",
    "</think>\n",
    "[Câu trả lời của bạn]\n",
    "\"\"\"\n",
    "prompt = '''Đáp án cuối cùng sau khi suy luận đã có, hãy phân tích để giải thích cho kết luận: '10 ngày' với câu hỏi dạng Tự luận.\n",
    "\n",
    "Dựa vào bối cảnh bên dưới, hãy phân tích kỹ trước khi trả lời câu hỏi.\n",
    "\n",
    "Loại câu hỏi: Tự luận\n",
    "\n",
    "Câu hỏi: Trong trường hợp các bên không có thỏa thuận khác hoặc quy tắc tố tụng của trung tâm trọng tài không có quy định khác, Trung tâm trọng tài phải gửi cho bị đơn bản sao đơn khởi kiện của nguyên đơn và những tài liệu theo quy định trong thời hạn bao lâu kể từ ngày nhận được đơn khởi kiện?\n",
    "\n",
    "Bối cảnh: \n",
    "Thông báo đơn khởi kiện\n",
    "\n",
    "Nếu các bên không có thoả thuận khác hoặc quy tắc tố tụng của Trung tâm trọng tài không có quy định khác, trong thời hạn 10 ngày, kể từ ngày nhận được đơn khởi kiện, các tài liệu kèm theo và chứng từ nộp tạm ứng phí trọng tài, Trung tâm trọng tài phải gửi cho bị đơn bản sao đơn khởi kiện của nguyên đơn và những tài liệu theo quy định tại khoản 3 Điều 30 của Luật này.\n",
    "\n",
    "Hãy đưa ra câu trả lời theo format sau:\n",
    "1. Phân tích câu hỏi: [trình bày ngắn gọn nội dung và ý định của câu hỏi]\n",
    "2. Dẫn chứng từ bối cảnh:\n",
    "   - Hãy tách từng đoạn dài trong bối cảnh thành nhiều ý nhỏ rõ ràng.\n",
    "   - Hãy tách ít nhất 3 đến 5 ý ở phần dẫn chứng từ bối cảnh\n",
    "   - Mỗi ý nên nêu rõ nội dung pháp lý, viết ngắn gọn dễ hiểu.\n",
    "   - Ví dụ:\n",
    "       - [ý 1 từ đoạn luật A]\n",
    "       - [ý 2 từ đoạn luật A]\n",
    "       - [ý 3 từ đoạn luật B]\n",
    "   - Ghi rõ đoạn nào có liên quan đến câu hỏi.\n",
    "3. Suy luận step-by-step:\n",
    "   a) [bước suy luận 1 dựa trên dẫn chứng ở trên]\n",
    "   b) [bước suy luận 2 tiếp theo]\n",
    "   …\n",
    "4. Kết luận: 10 ngày\n",
    "'''\n",
    "\n",
    "messages = [{'role': 'system', \n",
    "             'content': system_prompt}, \n",
    "            {'role': 'user', \n",
    "             'content': prompt}]\n",
    "response_text = generate_think_response(messages)\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:03:52.251769Z",
     "iopub.status.busy": "2025-07-18T03:03:52.251253Z",
     "iopub.status.idle": "2025-07-18T03:03:52.256652Z",
     "shell.execute_reply": "2025-07-18T03:03:52.256060Z",
     "shell.execute_reply.started": "2025-07-18T03:03:52.251749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "response_text.find('<think>'), response_text.find('</think>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:18:52.253863Z",
     "iopub.status.busy": "2025-07-18T03:18:52.253143Z",
     "iopub.status.idle": "2025-07-18T03:18:52.378671Z",
     "shell.execute_reply": "2025-07-18T03:18:52.377925Z",
     "shell.execute_reply.started": "2025-07-18T03:18:52.253839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load data\n",
    "with open('data/alqac25_train.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)\n",
    "\n",
    "with open('data/alqac25_law.json', 'r', encoding='utf-8') as f:\n",
    "    corpus = json.load(f)\n",
    "\n",
    "#Build lookup dictionary: { law_id: { article_id: text } }\n",
    "corpus_dict = {}\n",
    "for law in corpus:\n",
    "    law_id = law['id']\n",
    "    articles_dict = {article['id']: article['text'] for article in law.get('articles', [])}\n",
    "    corpus_dict[law_id] = articles_dict\n",
    "\n",
    "# Replace relevant_articles in questions\n",
    "for q in questions:\n",
    "    new_articles = []\n",
    "    for ref in q.get('relevant_articles', []):\n",
    "        law_id = ref['law_id']\n",
    "        article_id = ref['article_id']\n",
    "        text = corpus_dict.get(law_id, {}).get(article_id)\n",
    "        if text:\n",
    "            new_articles.append(text)\n",
    "    q['relevant_articles'] = new_articles  # Replace with list of strings\n",
    "\n",
    "# Save the updated file\n",
    "with open('data/alqac25_train_question_text.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:18:52.896635Z",
     "iopub.status.busy": "2025-07-18T03:18:52.896146Z",
     "iopub.status.idle": "2025-07-18T03:18:52.913788Z",
     "shell.execute_reply": "2025-07-18T03:18:52.913044Z",
     "shell.execute_reply.started": "2025-07-18T03:18:52.896613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/alqac25_train_question_text.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:18:53.754664Z",
     "iopub.status.busy": "2025-07-18T03:18:53.754143Z",
     "iopub.status.idle": "2025-07-18T03:18:53.759206Z",
     "shell.execute_reply": "2025-07-18T03:18:53.758653Z",
     "shell.execute_reply.started": "2025-07-18T03:18:53.754642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:18:54.430349Z",
     "iopub.status.busy": "2025-07-18T03:18:54.429597Z",
     "iopub.status.idle": "2025-07-18T03:18:54.536083Z",
     "shell.execute_reply": "2025-07-18T03:18:54.535376Z",
     "shell.execute_reply.started": "2025-07-18T03:18:54.430316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Bạn là một trợ lý pháp lý Tiếng Việt, có nhiệm vụ trả lời các câu hỏi ngắn gọn, chính xác, dựa trên nội dung điều luật được cung cấp. Chỉ trả lời bằng Tiếng Việt.\n",
    "\n",
    "Bạn hãy trả lời theo định dạng sau:\n",
    "<think>\n",
    "[Suy nghĩ, phân tích của bạn]\n",
    "</think>\n",
    "[Câu trả lời của bạn]\n",
    "\"\"\"\n",
    "def build_prompt(question: dict):\n",
    "    question_text, article_texts, question_type = question['text'], question['relevant_articles'], question['question_type']\n",
    "    texts = [a for a in article_texts if isinstance(a, str)]\n",
    "    article_content = \"\\n\\n\".join(texts)\n",
    "    if question_type == \"Tự luận\":\n",
    "        prompt = (\n",
    "            \"Dựa vào bối cảnh bên dưới, hãy phân tích kỹ trước khi trả lời câu hỏi.\\n\\n\"\n",
    "            f\"Loại câu hỏi: Tự luận\\n\\n\"\n",
    "            f\"Câu hỏi: {question_text}\\n\\n\"\n",
    "            f\"Bối cảnh: \\n{article_content}\\n\\n\"\n",
    "            \"Hãy sinh phần suy luận chi tiết theo mẫu bên dưới trong thẻ <think>...</think>. \"\n",
    "            \"Bạn cần viết đầy đủ các bước phân tích, dẫn chứng và suy luận trước khi đưa ra câu trả lời ngắn gọn.\\n\"\n",
    "            \"Không được trả lời ngay mà phải suy luận đầy đủ trước trong <think>.\\n\\n\"\n",
    "            \"<think>\\n\"\n",
    "            \"1. Phân tích câu hỏi: [trình bày ngắn gọn nội dung và ý định của câu hỏi]\\n\"\n",
    "            \"2. Dẫn chứng từ bối cảnh:\\n\"\n",
    "            \"   - Hãy tách từng đoạn dài trong bối cảnh thành nhiều ý nhỏ rõ ràng.\\n\"\n",
    "            \"   - Hãy tách ít nhất 3 đến 5 ý ở phần dẫn chứng từ bối cảnh\\n\"\n",
    "            \"   - Mỗi ý nên nêu rõ nội dung pháp lý, viết ngắn gọn dễ hiểu.\\n\"\n",
    "            \"   - Ví dụ:\\n\"\n",
    "            \"       - [ý 1 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 2 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 3 từ đoạn luật B]\\n\"\n",
    "            \"   - Ghi rõ đoạn nào có liên quan đến câu hỏi.\\n\"\n",
    "            \"3. Suy luận step-by-step:\\n\"\n",
    "            \"   a) [bước suy luận 1 dựa trên dẫn chứng ở trên]\\n\"\n",
    "            \"   b) [bước suy luận 2 tiếp theo]\\n\"\n",
    "            \"   …\\n\"\n",
    "            \"4. Kết luận: [tóm tắt câu trả lời cuối cùng dựa trên suy luận]\\n\"\n",
    "            \"</think>\\n\\n\"\n",
    "            \"[Kết luận cuối cùng sau khi suy luận]\"\n",
    "        )\n",
    "    elif question_type == \"Trắc nghiệm\":\n",
    "        prompt = (\n",
    "            \"Dựa vào bối cảnh bên dưới, hãy phân tích kỹ trước khi trả lời câu hỏi.\\n\\n\"\n",
    "            f\"Loại câu hỏi: Trắc nghiệm (format của Kết luận cuối cùng sau khi suy luận là 1 trong 4 kết luận: 'A', 'B', 'C', 'D'. Không được giải thích gì thêm.)\\n\\n\"\n",
    "            f\"Câu hỏi: {question_text}\\n\\n\"\n",
    "            f\"Bối cảnh: \\n{article_content}\\n\\n\"\n",
    "            f\"4 lựa chọn: \\n\"\n",
    "            f\"A: {q['choices']['A']}\\n\"\n",
    "            f\"B: {q['choices']['B']}\\n\"\n",
    "            f\"C: {q['choices']['C']}\\n\"\n",
    "            f\"D: {q['choices']['D']}\\n\"\n",
    "            \"\\n\\n\"\n",
    "            \"Hãy sinh phần suy luận chi tiết theo mẫu bên dưới trong thẻ <think>...</think>. \"\n",
    "            \"Bạn cần viết đầy đủ các bước phân tích, dẫn chứng và suy luận trước khi đưa ra câu trả lời ngắn gọn.\\n\"\n",
    "            \"Không được trả lời ngay mà phải suy luận đầy đủ trước trong <think>.\\n\\n\"\n",
    "            \"<think>\\n\"\n",
    "            \"1. Phân tích câu hỏi: [trình bày ngắn gọn nội dung và ý định của câu hỏi]\\n\"\n",
    "            \"2. Dẫn chứng từ bối cảnh:\\n\"\n",
    "            \"   - Hãy tách từng đoạn dài trong bối cảnh thành nhiều ý nhỏ rõ ràng.\\n\"\n",
    "            \"   - Hãy tách ít nhất 3 đến 5 ý ở phần dẫn chứng từ bối cảnh\\n\"\n",
    "            \"   - Mỗi ý nên nêu rõ nội dung pháp lý, viết ngắn gọn dễ hiểu.\\n\"\n",
    "            \"   - Ví dụ:\\n\"\n",
    "            \"       - [ý 1 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 2 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 3 từ đoạn luật B]\\n\"\n",
    "            \"   - Ghi rõ đoạn nào có liên quan đến câu hỏi.\\n\"\n",
    "            \"3. Suy luận step-by-step:\\n\"\n",
    "            \"   a) [bước suy luận 1 dựa trên dẫn chứng ở trên]\\n\"\n",
    "            \"   b) [bước suy luận 2 tiếp theo]\\n\"\n",
    "            \"   …\\n\"\n",
    "            \"4. Kết luận: [tóm tắt câu trả lời cuối cùng dựa trên suy luận]\\n\"\n",
    "            \"</think>\\n\\n\"\n",
    "            \"[Kết luận cuối cùng sau khi suy luận]\"\n",
    "        )\n",
    "    elif question_type == \"Đúng/Sai\":\n",
    "            prompt = (\n",
    "            \"Dựa vào bối cảnh bên dưới, hãy phân tích kỹ trước khi trả lời câu hỏi.\\n\\n\"\n",
    "            f\"Loại câu hỏi: Đúng/Sai (format của Kết luận cuối cùng sau khi suy luận là 1 trong 2 kết luận: 'Đúng', 'Sai'. Không được giải thích gì thêm.)\\n\\n\"\n",
    "            f\"Câu hỏi: {question_text}\\n\\n\"\n",
    "            f\"Bối cảnh: \\n{article_content}\\n\\n\"\n",
    "            \"Hãy sinh phần suy luận chi tiết theo mẫu bên dưới trong thẻ <think>...</think>. \"\n",
    "            \"Bạn cần viết đầy đủ các bước phân tích, dẫn chứng và suy luận trước khi đưa ra câu trả lời ngắn gọn.\\n\"\n",
    "            \"Không được trả lời ngay mà phải suy luận đầy đủ trước trong <think>.\\n\\n\"\n",
    "            \"<think>\\n\"\n",
    "            \"1. Phân tích câu hỏi: [trình bày ngắn gọn nội dung và ý định của câu hỏi]\\n\"\n",
    "            \"2. Dẫn chứng từ bối cảnh:\\n\"\n",
    "            \"   - Hãy tách từng đoạn dài trong bối cảnh thành nhiều ý nhỏ rõ ràng.\\n\"\n",
    "            \"   - Hãy tách ít nhất 3 đến 5 ý ở phần dẫn chứng từ bối cảnh\\n\"\n",
    "            \"   - Mỗi ý nên nêu rõ nội dung pháp lý, viết ngắn gọn dễ hiểu.\\n\"\n",
    "            \"   - Ví dụ:\\n\"\n",
    "            \"       - [ý 1 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 2 từ đoạn luật A]\\n\"\n",
    "            \"       - [ý 3 từ đoạn luật B]\\n\"\n",
    "            \"   - Ghi rõ đoạn nào có liên quan đến câu hỏi.\\n\"\n",
    "            \"3. Suy luận step-by-step:\\n\"\n",
    "            \"   a) [bước suy luận 1 dựa trên dẫn chứng ở trên]\\n\"\n",
    "            \"   b) [bước suy luận 2 tiếp theo]\\n\"\n",
    "            \"   …\\n\"\n",
    "            \"4. Kết luận: [tóm tắt câu trả lời cuối cùng dựa trên suy luận]\\n\"\n",
    "            \"</think>\\n\\n\"\n",
    "            \"[Kết luận cuối cùng sau khi suy luận]\"\n",
    "        )\n",
    "    else:\n",
    "        print('Lỗi')\n",
    "\n",
    "    return prompt\n",
    "\n",
    "for q in questions:\n",
    "    q['system_prompt'] = system_prompt\n",
    "    q['prompt'] = build_prompt(q)\n",
    "    gemini_text = f\"Kết luận cuối cùng sau khi suy luận đã có, hãy phân tích để giải thích cho kết luận: '{q['answer']}' với câu hỏi dạng {q['question_type']}.\\n\\n\"\n",
    "    q['think_prompt'] = gemini_text + q['prompt'].replace('[Kết luận cuối cùng sau khi suy luận]', q['answer']).replace('[tóm tắt câu trả lời cuối cùng dựa trên suy luận]', q['answer'])\n",
    "    # print(q['think_prompt'].find('Hãy sinh phần suy luận chi tiết theo mẫu bên dưới trong thẻ <think>...</think>. Bạn cần viết đầy đủ các bước phân tích, dẫn chứng và suy luận trước khi đưa ra câu trả lời ngắn gọn.\\nKhông được trả lời ngay mà phải suy luận đầy đủ trước trong <think>.\\n\\n<think>'))\n",
    "    q['think_prompt'] = q['think_prompt'].replace('Hãy sinh phần suy luận chi tiết theo mẫu bên dưới trong thẻ <think>...</think>. Bạn cần viết đầy đủ các bước phân tích, dẫn chứng và suy luận trước khi đưa ra câu trả lời ngắn gọn.\\nKhông được trả lời ngay mà phải suy luận đầy đủ trước trong <think>.\\n\\n<think>','Hãy đưa ra câu trả lời theo format sau:').split('</think>')[0]\n",
    "    q['think_prompt'] = q['think_prompt'].replace(\"(format của Kết luận cuối cùng sau khi suy luận là 1 trong 4 kết luận: 'A', 'B', 'C', 'D'. Không được giải thích gì thêm.)\",'')\n",
    "    q['think_prompt'] = q['think_prompt'].replace(\"(format của Kết luận cuối cùng sau khi suy luận là 1 trong 2 kết luận: 'Đúng', 'Sai'. Không được giải thích gì thêm.)\",'')\n",
    "\n",
    "with open('data/alqac25_train_question_text_new.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:18:57.123340Z",
     "iopub.status.busy": "2025-07-18T03:18:57.122589Z",
     "iopub.status.idle": "2025-07-18T03:18:57.179198Z",
     "shell.execute_reply": "2025-07-18T03:18:57.178470Z",
     "shell.execute_reply.started": "2025-07-18T03:18:57.123284Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('data/alqac25_train_question_text_new.json', 'r', encoding='utf-8') as f:\n",
    "    questions = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T18:37:02.804732Z",
     "iopub.status.busy": "2025-07-17T18:37:02.804164Z",
     "iopub.status.idle": "2025-07-17T18:37:02.807958Z",
     "shell.execute_reply": "2025-07-17T18:37:02.807376Z",
     "shell.execute_reply.started": "2025-07-17T18:37:02.804711Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# (questions[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:20:16.122250Z",
     "iopub.status.busy": "2025-07-18T03:20:16.121583Z",
     "iopub.status.idle": "2025-07-18T03:20:16.125251Z",
     "shell.execute_reply": "2025-07-18T03:20:16.124588Z",
     "shell.execute_reply.started": "2025-07-18T03:20:16.122226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(questions[50]['think_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-17T18:37:04.320333Z",
     "iopub.status.busy": "2025-07-17T18:37:04.319561Z",
     "iopub.status.idle": "2025-07-17T18:37:04.323304Z",
     "shell.execute_reply": "2025-07-17T18:37:04.322732Z",
     "shell.execute_reply.started": "2025-07-17T18:37:04.320305Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(questions[0]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen think answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:20:19.584811Z",
     "iopub.status.busy": "2025-07-18T03:20:19.584543Z",
     "iopub.status.idle": "2025-07-18T03:20:19.588154Z",
     "shell.execute_reply": "2025-07-18T03:20:19.587516Z",
     "shell.execute_reply.started": "2025-07-18T03:20:19.584793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/data_train_extracted/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-18T03:11:01.853424Z",
     "iopub.status.busy": "2025-07-18T03:11:01.852869Z",
     "iopub.status.idle": "2025-07-18T03:12:08.309193Z",
     "shell.execute_reply": "2025-07-18T03:12:08.308486Z",
     "shell.execute_reply.started": "2025-07-18T03:11:01.853401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import threading\n",
    "import queue\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from glob import glob\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CẤU HÌNH ===\n",
    "INPUT_FILE = 'data/alqac25_train_question_text_new.json'\n",
    "OUTPUT_DIR = 'data_train_extracted'\n",
    "\n",
    "# === LOGGING ===\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Tạo thư mục output nếu chưa tồn tại\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load JSON dataset\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    samples = json.load(f)\n",
    "\n",
    "# Thread-safe queue\n",
    "sample_queue = queue.Queue()\n",
    "for sample in samples:\n",
    "    sample_queue.put(sample)\n",
    "\n",
    "# # === MODEL CALL ===\n",
    "def generate_think_response(history: list[dict]) -> str: \n",
    "    response = think_model.chat(messages=history, sampling_params=sampling_params)\n",
    "    response_text = response[0].outputs[0].text\n",
    "    return response_text\n",
    "\n",
    "# === CHẠY BÌNH THƯỜNG KHÔNG THREAD ===\n",
    "def run_single_thread():\n",
    "    i = 0\n",
    "    for sample in tqdm(samples):\n",
    "        i += 1\n",
    "        # if i > 5:\n",
    "        #     break\n",
    "        try:\n",
    "            sid = sample[\"question_id\"]\n",
    "            out_path = os.path.join(OUTPUT_DIR, f\"{sid}.json\")\n",
    "    \n",
    "            # ✅ Bỏ qua nếu file đã tồn tại\n",
    "            if os.path.exists(out_path): #########################\n",
    "            # if os.path.exists(out_path) or out_path.split('/')[-1] in runned_files:\n",
    "                continue\n",
    "    \n",
    "            history = [\n",
    "                {\"role\": \"system\", \"content\": sample[\"system_prompt\"]},\n",
    "                {\"role\": \"user\", \"content\": sample[\"think_prompt\"]}\n",
    "            ]\n",
    "            # print('historyyyyyyyyyyy',i,history)\n",
    "\n",
    "            try:\n",
    "                # answer = generate_gemini_response(api_key, history, logger)\n",
    "                answer = generate_think_response(history)\n",
    "                match = re.search(r'<think>(.*?)</think>', answer, re.DOTALL) # re.DOTALL để khớp cả ký tự xuống dòng\n",
    "                # print(answer)\n",
    "                if match:\n",
    "                    # think_content = \"<think>\" + match.group(1) + \"</think>\" # Thêm lại thẻ <think> và </think> nếu bạn muốn\n",
    "                    think_content = answer.split('</think>')[-1]\n",
    "                else:\n",
    "                    print(f'❌ Lỗi <think> trong file {sample[\"question_id\"]} với answer là: {answer}')\n",
    "                    continue\n",
    "                processed_answer = '<think>\\n' + think_content + '\\n</think>\\n' +sample['answer']\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"[{sid}] Lỗi khi gọi model: {e}\")\n",
    "                continue  # ❌ Bỏ qua nếu lỗi\n",
    "                \n",
    "            # print('answer', answer)\n",
    "            # print('processed_answer', processed_answer)\n",
    "    \n",
    "            # ✅ Lưu đủ 6 key: id, text, relevant_articles, system_prompt, prompt, answer\n",
    "            output_data = {\n",
    "                \"question_id\": sample[\"question_id\"],\n",
    "                \"question_type\": sample[\"question_type\"],\n",
    "                \"text\": sample[\"text\"],\n",
    "                \"relevant_articles\": sample[\"relevant_articles\"],\n",
    "                \"answer\": sample[\"answer\"],\n",
    "                \"system_prompt\": sample[\"system_prompt\"],\n",
    "                \"prompt\": sample[\"prompt\"],\n",
    "                \"answer_think\": processed_answer,\n",
    "            }\n",
    "    \n",
    "            with open(out_path, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(output_data, out_f, ensure_ascii=False, indent=2)\n",
    "            print(f'Đã lưu file thứ {i}: {out_path}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'❌ Lỗi trong file {sample[\"question_id\"]}: {e}')\n",
    "\n",
    "    logger.info(f\"✅ Hoàn thành tuần tự. Kết quả lưu tại: {OUTPUT_DIR}\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    run_single_thread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-17T06:50:42.592891Z",
     "iopub.status.idle": "2025-07-17T06:50:42.593233Z",
     "shell.execute_reply": "2025-07-17T06:50:42.593083Z",
     "shell.execute_reply.started": "2025-07-17T06:50:42.593069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# === CẤU HÌNH ===\n",
    "INPUT_FILE = 'data/alqac25_train_question_text_new.json'\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    samples = json.load(f)\n",
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge json files (nhớ lấy các lựa chọn trắc nghiệm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def combine_json_files(input_directory, output_file_name):\n",
    "    \"\"\"\n",
    "    Gộp tất cả các file JSON trong một thư mục thành một file JSON duy nhất.\n",
    "\n",
    "    Args:\n",
    "        input_directory (str): Đường dẫn đến thư mục chứa các file JSON.\n",
    "        output_file_name (str): Tên của file JSON đầu ra.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    # Kiểm tra xem thư mục đầu vào có tồn tại không\n",
    "    if not os.path.isdir(input_directory):\n",
    "        print(f\"Lỗi: Thư mục '{input_directory}' không tồn tại.\")\n",
    "        return\n",
    "\n",
    "    # Duyệt qua tất cả các file trong thư mục\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(input_directory, filename)\n",
    "            try:\n",
    "                with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    all_data.append(data)\n",
    "                print(f\"Đã đọc file: {filename}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Lỗi đọc JSON từ file '{filename}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi không xác định khi xử lý file '{filename}': {e}\")\n",
    "\n",
    "    # Ghi tất cả dữ liệu vào một file JSON duy nhất\n",
    "    if all_data:\n",
    "        try:\n",
    "            with open(output_file_name, 'w', encoding='utf-8') as f:\n",
    "                json.dump(all_data, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Đã gộp thành công {len(all_data)} file vào '{output_file_name}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi ghi file đầu ra '{output_file_name}': {e}\")\n",
    "    else:\n",
    "        print(f\"Không tìm thấy file JSON nào trong thư mục '{input_directory}' để gộp.\")\n",
    "\n",
    "# Ví dụ sử dụng:\n",
    "# Giả sử các file JSON của bạn nằm trong thư mục có tên 'data_zalo_extracted'\n",
    "input_folder = \"data_train_extracted\" \n",
    "output_json_file = \"data_train_legal_qa.json\"\n",
    "\n",
    "combine_json_files(input_folder, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define file paths\n",
    "main_input_file = 'data_train_legal_qa.json'\n",
    "source_choices_file = 'data/alqac25_train_question_text_new.json'\n",
    "output_file = 'data_train_legal_qa_new.json'\n",
    "\n",
    "try:\n",
    "    # 1. Load the main data file\n",
    "    with open(main_input_file, 'r', encoding='utf-8') as f:\n",
    "        main_data = json.load(f)\n",
    "    print(f\"Đã đọc {len(main_data)} mục từ file '{main_input_file}'.\")\n",
    "\n",
    "    # 2. Load the source file containing choices\n",
    "    with open(source_choices_file, 'r', encoding='utf-8') as f:\n",
    "        source_data = json.load(f)\n",
    "    print(f\"Đã đọc {len(source_data)} mục từ file '{source_choices_file}'.\")\n",
    "\n",
    "    # 3. Create a lookup dictionary for choices from the source data\n",
    "    choices_lookup = {}\n",
    "    for item in source_data:\n",
    "        if item.get('question_type') == \"Trắc nghiệm\" and 'choices' in item:\n",
    "            choices_lookup[item['question_id']] = item['choices']\n",
    "    print(f\"Đã tạo lookup dictionary với {len(choices_lookup)} mục lựa chọn.\")\n",
    "\n",
    "    # 4. Iterate through the main data and add choices\n",
    "    merged_count = 0\n",
    "    for item in main_data:\n",
    "        if item.get('question_type') == \"Trắc nghiệm\":\n",
    "            question_id = item.get('question_id')\n",
    "            if question_id in choices_lookup:\n",
    "                if 'choices' not in item: # Only add if 'choices' key doesn't exist\n",
    "                    item['choices'] = choices_lookup[question_id]\n",
    "                    merged_count += 1\n",
    "                else:\n",
    "                    print(f\"Cảnh báo: Mục '{question_id}' đã có khóa 'choices'. Bỏ qua.\")\n",
    "            else:\n",
    "                print(f\"Cảnh báo: Không tìm thấy lựa chọn cho 'Trắc nghiệm' question_id: {question_id} trong file nguồn.\")\n",
    "\n",
    "    # 5. Save the modified data to a new JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(main_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nĐã hợp nhất 'choices' cho {merged_count} mục loại 'Trắc nghiệm'.\")\n",
    "    print(f\"Dữ liệu đã được lưu vào file mới: '{output_file}'\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Lỗi: Không tìm thấy file {e.filename}. Vui lòng kiểm tra lại đường dẫn.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Lỗi: Không thể giải mã JSON từ file. Đảm bảo file có định dạng JSON hợp lệ. Chi tiết: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Đã xảy ra lỗi không mong muốn: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => file data_train_legal_qa_new.json là file dùng để training"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
